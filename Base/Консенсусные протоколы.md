## Что такое консенсусные протоколы
aliases: 
	- "Консенсусные протоколы"

---

# Консенсусные протоколы

## Основная идея консенсуса

В самом общем виде **консенсус** означает:

1. **Узлы предлагают** разные варианты (значения, команды, блоки журнала, кандидатов в лидеры и т.п.).
2. **Алгоритм выбирает** ровно **одно** из предложенных значений.
3. **Все узлы**, которые не вышли из строя, в итоге **соглашаются** с выбранным вариантом (система достигает единого решения).
4. **Решение неотменяемо**: если узлы «приняли» значение, то они не могут потом взять и выбрать другое вместо него.

Также в большинстве реализаций важно требование:

- **Живучесть (liveness)**: если более половины узлов (или нужный кворум) доступны и работают, система должна продолжать принимать решения, не зависая навечно.

Проблема в том, что узлы могут **выходить из строя**, а сеть — рваться. При этом алгоритм не должен «выбрать два варианта» (то есть не допустить split-brain), а также не должен «навсегда зависать», если хотя бы большинство узлов функционируют.

## Алгоритмы консенсуса

- [[Paxos]] -  алгоритм [консенсуса в распределённой системе], который детерминированно работает в асинхронной системе с отказами узлов, гарантирует корректный консенсус, но не гарантирует, что тот при наличии отказов будет достигнут на конечное время
- [[Raft]]
- [[Zab]]
### Чем отличаются Paxos, Raft и Zab
- **Paxos** (в классической статье) описывает формальный консенсус, но мало рассказывает о _деталях_ реализации журналов, про смену лидера и т.п. Часто говорят «Paxos сложен в реализации».
- **Multi-Paxos** добавляет «лидера» для оптимизации. Raft, по сути, очень похожа на Multi-Paxos, но подаётся как более «читаемый» алгоритм.
- **Zab** ориентирован конкретно на поток операций (atomic broadcast), на котором построен ZooKeeper. По структуре похож на Raft/Viewstamped Replication.

Все три дают схожие гарантии: _корректное_ принятие решений при сбое узлов и сетевых задержках, если есть работающее большинство узлов.

#### Дополнительный материал про алгоритмы консенсуса и не только:
- [Кратко про Raft и Paxos](https://habr.com/ru/companies/otus/articles/793198/)
- [Hadoop. ZooKeeper](https://habr.com/ru/articles/487058/)
- [Как сервера договариваются друг с другом: алгоритм распределённого консенсуса Raft](https://habr.com/ru/companies/dododev/articles/469999/)
- [Алгоритм консенсуса RAFT](https://www.youtube.com/watch?v=4rwAUKH4bJk&ab_channel=BPMN%2C%D0%91%D0%B8%D0%B7%D0%BD%D0%B5%D1%81-%D0%BF%D1%80%D0%BE%D1%86%D0%B5%D1%81%D1%81%D1%8B%D0%B8%D0%BA%D0%BE%D1%82%D0%B8%D0%BA%D0%B8)

---

## Применение консенсуса

1. **Распределённая база данных**
    
    - Хранение общего журнала (sequence of writes) так, чтобы все реплики применяли одни и те же записи в одном и том же порядке.
    - Пример: Google Spanner использует Paxos, etcd/TiKV — Raft, Apache ZooKeeper — Zab.
2. **Сервис выбора лидера и координации**
    
    - ZooKeeper: с помощью Zab реализует линеаризуемые операции (create, set), благодаря чему можно строить распределённые блокировки, «учёт» активных узлов и т. д.
3. **Сообщения в одном порядке (Atomic Broadcast)**
    
    - Kafka (внутри не совсем классический консенсус, но идея похожа — leader + репликация журнала).
    - Пригодно для распределённых очередей, транзакционных журналов.
4. **Файловые системы или настройки**
    
    - Ceph, GlusterFS, HDFS JournalNode (NameNode в режиме HA) применяют протоколы типа Paxos или Raft.

---
## Основные проблемы и ограничения

- **Сложность реализации**. При написании собственного алгоритма легко допустить нетривиальные ошибки, проявляющиеся только при сбоях/сетевых задержках. Именно поэтому советуют использовать готовые библиотеки (etcd, ZK).
- **Чувствительность к задержкам сети**: если сеть между узлами работает плохо (пакеты теряются, большие RTT), часто будут происходить «ложные» перевыборы лидера. Система в эти моменты почти ничего не делает, кроме выборов.
- **Остановка при разделении сети**: при split-brain только одна сторона (имеющая большинство) может продолжать принимать записи, другая — блокируется.
- **Непростое изменение состава кластера** (динамическое членство): чтобы добавлять/удалять узлы, тоже нужно особое согласование (по сути, «двухэтапная смена конфигурации»), и это опять повышает риск ошибиться.
- **Монотонное увеличение журнала**: в долгоживущих системах журнал может расти. Нужно периодически «снимать снимки» (snapshot), чтобы не хранить все записи.

---
## Краткое резюме

**Консенсусные протоколы** — это фундамент надёжного _распределённого_ хранения и управления состоянием. На практике главные варианты:

- **Paxos** — классическая модель, доказана корректность, но относительно «труден» в голом виде.
- **Raft** — упрощённое (концептуально) семейство идей Paxos, чётко описанная процедура выбора лидера и репликации лога, используется в etcd, Consul, TiKV и др.
- **Zab** — протокол в ZooKeeper для упорядоченной рассылки (atomic broadcast), фактически очень похож на Raft/Viewstamped Replication.

Все они решают одну задачу: _как несколько узлов могут согласованно принимать одно и то же решение в условиях, когда некоторые из узлов или связь могут временно отказывать_. Для прикладного разработчика ключевой вывод:

- Если нужно «настоящий» консенсус (выбор лидера, полная строгая согласованность журнала) — лучше использовать готовые системы (ZooKeeper, etcd, Consul), чем писать с нуля.
- Консенсус **всегда** означает _синхронное_ ожидание кворума. Поэтому он дороже по задержкам, чем ассинхронные решения, но зато гарантирует отсутствие «раздвоения» и потери зафиксированных данных.
---
Tags: #highLoad #develop #coherence  #consensus
Author: [[]]
Related: [[Линеаризуемость]]
URL: -- 